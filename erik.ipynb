{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# API\n",
    "import requests\n",
    "\n",
    "# Automating\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-10-18 16:12:14.501061, 4490 records, 1000 returned per call, 5 iterations needed.\n",
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n",
      "Iteration 4 done\n",
      "Iteration 5 done\n",
      "END 2020-10-18 16:12:16.685142, 4490 records in file\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Paging example using Python 3. Output in JSON.\n",
    "\n",
    "\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Base URL for this endpoint. Add filters, column selection, and sort order to this.\n",
    "baseUrl = \"https://www.fema.gov/api/open/v1/FemaWebDisasterDeclarations?\"\n",
    "\n",
    "\n",
    "top = 1000      # number of records to get per call\n",
    "skip = 0        # number of records to skip\n",
    "\n",
    "\n",
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "webUrl = urllib.request.urlopen(baseUrl + \"$inlinecount=allpages&$select=id&$top=1\")\n",
    "result = webUrl.read()\n",
    "jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "\n",
    "# send some logging info to the console so we know what is happening\n",
    "print(\"START \" + str(datetime.now()) + \", \" + str(recCount) + \" records, \" + str(top) + \" returned per call, \" + str(loopNum) + \" iterations needed.\")\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"FemaWebDisasterDeclarations.json\", \"a\")\n",
    "outFile.write('{\"femawebdisasterdeclarations\":[')\n",
    "\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    webUrl = urllib.request.urlopen(baseUrl + \"&$metadata=off&$format=jsona&$skip=\" + str(skip) + \"&$top=\" + str(top))\n",
    "    result = webUrl.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "\n",
    "outFile.close()\n",
    "\n",
    "\n",
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"FemaWebDisasterDeclarations.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(\"END \" + str(datetime.now()) + \", \" + str(len(my_data['femawebdisasterdeclarations'])) + \" records in file\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-10-22 21:35:57.674976, 3250 records, 1000 returned per call, 4 iterations needed.\n",
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n",
      "Iteration 4 done\n",
      "END 2020-10-22 21:35:59.065228, 3250 records in file\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Paging example using Python 3. Output in JSON.\n",
    "\n",
    "\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Base URL for this endpoint. Add filters, column selection, and sort order to this.\n",
    "baseUrl = \"https://www.fema.gov/api/open/v1/FemaWebDisasterSummaries?\"\n",
    "\n",
    "\n",
    "top = 1000      # number of records to get per call\n",
    "skip = 0        # number of records to skip\n",
    "\n",
    "\n",
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "webUrl = urllib.request.urlopen(baseUrl + \"$inlinecount=allpages&$select=id&$top=1\")\n",
    "result = webUrl.read()\n",
    "jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "\n",
    "# send some logging info to the console so we know what is happening\n",
    "print(\"START \" + str(datetime.now()) + \", \" + str(recCount) + \" records, \" + str(top) + \" returned per call, \" + str(loopNum) + \" iterations needed.\")\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"FemaWebDisasterSummaries.json\", \"a\")\n",
    "outFile.write('{\"FemaWebDisasterSummaries\":[')\n",
    "\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    webUrl = urllib.request.urlopen(baseUrl + \"&$metadata=off&$format=jsona&$skip=\" + str(skip) + \"&$top=\" + str(top))\n",
    "    result = webUrl.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "\n",
    "outFile.close()\n",
    "\n",
    "\n",
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"FemaWebDisasterSummaries.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(\"END \" + str(datetime.now()) + \", \" + str(len(my_data['FemaWebDisasterSummaries'])) + \" records in file\")\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-10-22 21:40:59.032736, 1750 records, 1000 returned per call, 2 iterations needed.\n",
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "END 2020-10-22 21:40:59.907441, 1750 records in file\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Paging example using Python 3. Output in JSON.\n",
    "\n",
    "\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Base URL for this endpoint. Add filters, column selection, and sort order to this.\n",
    "baseUrl = \"https://www.fema.gov/api/open/v1/HazardMitigationGrantProgramDisasterSummaries?\"\n",
    "\n",
    "\n",
    "top = 1000      # number of records to get per call\n",
    "skip = 0        # number of records to skip\n",
    "\n",
    "\n",
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "webUrl = urllib.request.urlopen(baseUrl + \"$inlinecount=allpages&$select=id&$top=1\")\n",
    "result = webUrl.read()\n",
    "jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "\n",
    "# send some logging info to the console so we know what is happening\n",
    "print(\"START \" + str(datetime.now()) + \", \" + str(recCount) + \" records, \" + str(top) + \" returned per call, \" + str(loopNum) + \" iterations needed.\")\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"HazardMitigationGrantProgramDisasterSummaries.json\", \"a\")\n",
    "outFile.write('{\"HazardMitigationGrantProgramDisasterSummaries\":[')\n",
    "\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    webUrl = urllib.request.urlopen(baseUrl + \"&$metadata=off&$format=jsona&$skip=\" + str(skip) + \"&$top=\" + str(top))\n",
    "    result = webUrl.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "\n",
    "outFile.close()\n",
    "\n",
    "\n",
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"HazardMitigationGrantProgramDisasterSummaries.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(\"END \" + str(datetime.now()) + \", \" + str(len(my_data['HazardMitigationGrantProgramDisasterSummaries'])) + \" records in file\")\n",
    "inFile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2020-10-22 21:53:41.199543, 60276 records, 1000 returned per call, 61 iterations needed.\n",
      "Iteration 1 done\n",
      "Iteration 2 done\n",
      "Iteration 3 done\n",
      "Iteration 4 done\n",
      "Iteration 5 done\n",
      "Iteration 6 done\n",
      "Iteration 7 done\n",
      "Iteration 8 done\n",
      "Iteration 9 done\n",
      "Iteration 10 done\n",
      "Iteration 11 done\n",
      "Iteration 12 done\n",
      "Iteration 13 done\n",
      "Iteration 14 done\n",
      "Iteration 15 done\n",
      "Iteration 16 done\n",
      "Iteration 17 done\n",
      "Iteration 18 done\n",
      "Iteration 19 done\n",
      "Iteration 20 done\n",
      "Iteration 21 done\n",
      "Iteration 22 done\n",
      "Iteration 23 done\n",
      "Iteration 24 done\n",
      "Iteration 25 done\n",
      "Iteration 26 done\n",
      "Iteration 27 done\n",
      "Iteration 28 done\n",
      "Iteration 29 done\n",
      "Iteration 30 done\n",
      "Iteration 31 done\n",
      "Iteration 32 done\n",
      "Iteration 33 done\n",
      "Iteration 34 done\n",
      "Iteration 35 done\n",
      "Iteration 36 done\n",
      "Iteration 37 done\n",
      "Iteration 38 done\n",
      "Iteration 39 done\n",
      "Iteration 40 done\n",
      "Iteration 41 done\n",
      "Iteration 42 done\n",
      "Iteration 43 done\n",
      "Iteration 44 done\n",
      "Iteration 45 done\n",
      "Iteration 46 done\n",
      "Iteration 47 done\n",
      "Iteration 48 done\n",
      "Iteration 49 done\n",
      "Iteration 50 done\n",
      "Iteration 51 done\n",
      "Iteration 52 done\n",
      "Iteration 53 done\n",
      "Iteration 54 done\n",
      "Iteration 55 done\n",
      "Iteration 56 done\n",
      "Iteration 57 done\n",
      "Iteration 58 done\n",
      "Iteration 59 done\n",
      "Iteration 60 done\n",
      "Iteration 61 done\n",
      "END 2020-10-22 21:54:06.695558, 60276 records in file\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Paging example using Python 3. Output in JSON.\n",
    "\n",
    "\n",
    "import sys\n",
    "import urllib.request\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Base URL for this endpoint. Add filters, column selection, and sort order to this.\n",
    "baseUrl = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries?\"\n",
    "\n",
    "\n",
    "top = 1000      # number of records to get per call\n",
    "skip = 0        # number of records to skip\n",
    "\n",
    "\n",
    "# Return 1 record with your criteria to get total record count. Specifying only 1\n",
    "#   column here to reduce amount of data returned. Need inlinecount to get record count. \n",
    "webUrl = urllib.request.urlopen(baseUrl + \"$inlinecount=allpages&$select=id&$top=1\")\n",
    "result = webUrl.read()\n",
    "jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "# calculate the number of calls we will need to get all of our data (using the maximum of 1000)\n",
    "recCount = jsonData['metadata']['count']\n",
    "loopNum = math.ceil(recCount / top)\n",
    "\n",
    "\n",
    "# send some logging info to the console so we know what is happening\n",
    "print(\"START \" + str(datetime.now()) + \", \" + str(recCount) + \" records, \" + str(top) + \" returned per call, \" + str(loopNum) + \" iterations needed.\")\n",
    "\n",
    "\n",
    "# Initialize our file. Only doing this because of the type of file wanted. See the loop below.\n",
    "#   The root json entity is usually the name of the dataset, but you can use any name.\n",
    "outFile = open(\"DisasterDeclarationsSummaries.json\", \"a\")\n",
    "outFile.write('{\"DisasterDeclarationsSummaries\":[')\n",
    "\n",
    "\n",
    "# Loop and call the API endpoint changing the record start each iteration. The metadata is being\n",
    "#   suppressed as we no longer need it.\n",
    "i = 0\n",
    "while (i < loopNum):\n",
    "    # By default data is returned as a JSON object, the data set name being the root element. Unless\n",
    "    #   you extract records as you process, you will end up with 1 distinct JSON object for EVERY \n",
    "    #   call/iteration. An alternative is to return the data as JSONA (an array of json objects) with \n",
    "    #   no root element - just a bracket at the start and end. This is easier to manipulate.\n",
    "    webUrl = urllib.request.urlopen(baseUrl + \"&$metadata=off&$format=jsona&$skip=\" + str(skip) + \"&$top=\" + str(top))\n",
    "    result = webUrl.read()\n",
    "    \n",
    "    # The data is already returned in a JSON format. There is no need to decode and load as a JSON object.\n",
    "    #   If you want to begin working with and manipulating the JSON, import the json library and load with\n",
    "    #   something like: jsonData = json.loads(result.decode())\n",
    "\n",
    "\n",
    "    # Append results to file, trimming off first and last JSONA brackets, adding comma except for last call,\n",
    "    #   AND root element terminating array bracket and brace to end unless on last call. The goal here is to \n",
    "    #   create a valid JSON file that contains ALL the records. This can be done differently.\n",
    "    if (i == (loopNum - 1)):\n",
    "        # on the last so terminate the single JSON object\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \"]}\")\n",
    "    else:\n",
    "        outFile.write(str(result[1:-1],'utf-8') + \",\")\n",
    "\n",
    "\n",
    "    # increment the loop counter and skip value\n",
    "    i+=1\n",
    "    skip = i * top\n",
    "\n",
    "\n",
    "    print(\"Iteration \" + str(i) + \" done\")\n",
    "\n",
    "\n",
    "outFile.close()\n",
    "\n",
    "\n",
    "# lets re-open the file and see if we got the number of records we expected\n",
    "inFile = open(\"DisasterDeclarationsSummaries.json\", \"r\")\n",
    "my_data = json.load(inFile)\n",
    "print(\"END \" + str(datetime.now()) + \", \" + str(len(my_data['DisasterDeclarationsSummaries'])) + \" records in file\")\n",
    "inFile.close()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
